name: URL Analyzer & Structure Detection

on:
  workflow_dispatch:
    inputs:
      force_refresh:
        description: 'Force refresh of URL analysis'
        required: false
        default: false
        type: boolean
  schedule:
    # Run weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'

permissions:
  contents: write
  actions: read

jobs:
  analyze-urls:
    runs-on: ubuntu-latest
    
    outputs:
      analysis-file: ${{ steps.analysis.outputs.analysis-file }}
      total-newspapers: ${{ steps.analysis.outputs.total-newspapers }}
      total-files: ${{ steps.analysis.outputs.total-files }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml requests beautifulsoup4 urllib3

      - name: Analyze URLs and detect structure
        id: analysis
        run: |
          echo "ðŸ” Starting URL analysis and structure detection..."
          
          python -c "
          import yaml
          import json
          import requests
          from urllib.parse import urljoin, urlparse
          from datetime import datetime
          import sys
          import os
          from pathlib import Path
          
          def analyze_url_structure(url):
              try:
                  response = requests.head(url, timeout=10, allow_redirects=True)
                  content_type = response.headers.get('content-type', '').lower()
                  
                  if 'pdf' in content_type:
                      return 'direct_file', response.headers.get('content-length', 0)
                  elif 'html' in content_type:
                      return 'directory', None
                  else:
                      response = requests.get(url, timeout=10, stream=True)
                      content_type = response.headers.get('content-type', '').lower()
                      
                      if 'pdf' in content_type:
                          return 'direct_file', response.headers.get('content-length', 0)
                      else:
                          return 'directory', None
                          
              except Exception as e:
                  print(f'Error analyzing {url}: {e}')
                  return 'unknown', None
          
          def extract_newspaper_name_from_url(url):
              import re
              
              parsed = urlparse(url)
              path_parts = [part for part in parsed.path.split('/') if part]
              
              persian_names = {
                  'neshat': 'Ù†Ø´Ø§Ø·',
                  'tous': 'ØªÙˆØ³', 
                  'jameh': 'Ø¬Ø§Ù…Ø¹Ù‡',
                  'kayhan': 'Ú©ÛŒÙ‡Ø§Ù†',
                  'ettelaat': 'Ø§Ø·Ù„Ø§Ø¹Ø§Øª',
                  'iran': 'Ø§ÛŒØ±Ø§Ù†',
                  'hamshahri': 'Ù‡Ù…Ø´Ù‡Ø±ÛŒ',
                  'shargh': 'Ø´Ø±Ù‚'
              }
              
              for part in path_parts:
                  part_lower = part.lower()
                  for eng_name, persian_name in persian_names.items():
                      if eng_name in part_lower or persian_name in part:
                          return persian_name, eng_name
                  
                  if re.search(r'(neshat|tous|jameh|kayhan)', part_lower):
                      match = re.search(r'(neshat|tous|jameh|kayhan)', part_lower)
                      eng_name = match.group(1)
                      return persian_names.get(eng_name, eng_name), eng_name
              
              return None, None
          
          # Load URLs configuration
          with open('urls.yml', 'r', encoding='utf-8') as f:
              config = yaml.safe_load(f)
          
          analysis_results = {
              'analysis_date': datetime.now().isoformat(),
              'newspapers': {},
              'summary': {
                  'total_archives': 0,
                  'total_files': 0,
                  'direct_files': 0,
                  'directories': 0,
                  'unknown': 0
              }
          }
          
          for archive in config.get('archives', []):
              archive_name = archive.get('folder', 'unknown')
              newspaper_fa = archive.get('source_info', {}).get('newspaper_name', '')
              
              if not newspaper_fa:
                  title_fa = archive.get('title_fa', '')
                  if 'Ø¢Ø±Ø´ÛŒÙˆ' in title_fa:
                      newspaper_fa = title_fa.replace('Ø¢Ø±Ø´ÛŒÙˆ', '').replace('Ù†Ø´Ø±ÛŒÙ‡', '').strip()
              
              newspaper_info = {
                  'title_fa': archive.get('title_fa', ''),
                  'title_en': archive.get('title_en', ''),
                  'folder': archive_name,
                  'category': archive.get('category', 'old-newspaper'),
                  'description': archive.get('description', ''),
                  'newspaper_name': newspaper_fa,
                  'files': [],
                  'total_files': 0,
                  'years': {}
              }
              
              for year, urls in archive.get('years', {}).items():
                  year_files = []
                  
                  for url in urls:
                      print(f'Analyzing: {url}')
                      url_type, size = analyze_url_structure(url)
                      
                      if not newspaper_fa:
                          persian_name, eng_name = extract_newspaper_name_from_url(url)
                          if persian_name:
                              newspaper_info['newspaper_name'] = persian_name
                              if not newspaper_info['title_fa']:
                                  newspaper_info['title_fa'] = f'Ø¢Ø±Ø´ÛŒÙˆ Ù†Ø´Ø±ÛŒÙ‡ {persian_name}'
                              if not newspaper_info['title_en']:
                                  newspaper_info['title_en'] = f'{eng_name.title()} Archive'
                      
                      file_info = {
                          'url': url,
                          'type': url_type,
                          'size': size,
                          'year': year,
                          'filename': f'{archive_name}_{len(year_files)+1:03d}.pdf'
                      }
                      
                      year_files.append(file_info)
                      newspaper_info['files'].append(file_info)
                      
                      analysis_results['summary']['total_files'] += 1
                      if url_type == 'direct_file':
                          analysis_results['summary']['direct_files'] += 1
                      elif url_type == 'directory':
                          analysis_results['summary']['directories'] += 1
                      else:
                          analysis_results['summary']['unknown'] += 1
                  
                  newspaper_info['years'][year] = year_files
              
              newspaper_info['total_files'] = len(newspaper_info['files'])
              analysis_results['newspapers'][archive_name] = newspaper_info
              analysis_results['summary']['total_archives'] += 1
          
          # Save analysis results
          analysis_file = f'url_analysis_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'
          with open(analysis_file, 'w', encoding='utf-8') as f:
              json.dump(analysis_results, f, ensure_ascii=False, indent=2)
          
          print(f'Analysis complete! Results saved to {analysis_file}')
          print(f'Total newspapers: {analysis_results[\"summary\"][\"total_archives\"]}')
          print(f'Total files: {analysis_results[\"summary\"][\"total_files\"]}')
          print(f'Direct files: {analysis_results[\"summary\"][\"direct_files\"]}')
          
          # Set outputs for next workflow
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'analysis-file={analysis_file}\\n')
              f.write(f'total-newspapers={analysis_results[\"summary\"][\"total_archives\"]}\\n')
              f.write(f'total-files={analysis_results[\"summary\"][\"total_files\"]}\\n')
          "

      - name: Generate README structure
        run: |
          echo "ðŸ“ Generating README structure for each newspaper..."
          
          python -c "
          import json
          import os
          from pathlib import Path
          
          # Find the latest analysis file
          analysis_files = [f for f in os.listdir('.') if f.startswith('url_analysis_')]
          if not analysis_files:
              print('No analysis file found!')
              exit(1)
          
          latest_analysis = sorted(analysis_files)[-1]
          
          with open(latest_analysis, 'r', encoding='utf-8') as f:
              analysis = json.load(f)
          
          # Create directory structure and README templates
          for newspaper_name, info in analysis['newspapers'].items():
              newspaper_dir = Path(f'old-newspaper/{newspaper_name}')
              newspaper_dir.mkdir(parents=True, exist_ok=True)
              
              # Generate README content
              readme_lines = [
                  '[ðŸ‡ºðŸ‡¸ English](README.en.md) | ðŸ‡®ðŸ‡· ÙØ§Ø±Ø³ÛŒ',
                  '',
                  f'# {info[\"title_fa\"]} / {info[\"title_en\"]}',
                  '',
                  info['description'],
                  '',
                  '## Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ / Available Issues',
                  ''
              ]
              
              # Add years and file counts
              for year, files in info['years'].items():
                  readme_lines.append(f'- **{year}**: {len(files)} issues ([View folder]({year}/))')
              
              readme_lines.extend([
                  '',
                  '## Ø¢Ù…Ø§Ø± / Statistics',
                  '',
                  f'- **ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ / Total Files**: {info[\"total_files\"]}',
                  f'- **Ø³Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ / Available Years**: {\", \".join(info[\"years\"].keys())}',
                  '',
                  '## Ù…Ù†Ø¨Ø¹ / Source',
                  '',
                  f'**Ù†Ø§Ù… Ù†Ø´Ø±ÛŒÙ‡ / Publication Name:** {info[\"newspaper_name\"]}',
                  '',
                  '**Ø±ÙˆØ´ Ø¯Ø§Ù†Ù„ÙˆØ¯ / Download Method:** Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø³ÛŒØ³ØªÙ… Ø¢Ø±Ø´ÛŒÙˆ / Automatic via archive system',
                  '',
                  '---',
                  '*ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙˆØ³Ø· Ø³ÛŒØ³ØªÙ… Ø¢Ø±Ø´ÛŒÙˆ Ø§ÛŒØ±Ø§Ù† / Generated automatically by Iranian Archive Workflow*',
                  f'*Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ / Last Updated: {analysis[\"analysis_date\"]}*'
              ])
              
              readme_content = '\\n'.join(readme_lines)
              
              # Write README
              readme_path = newspaper_dir / 'README.md'
              with open(readme_path, 'w', encoding='utf-8') as f:
                  f.write(readme_content)
              
              print(f'Generated README for {newspaper_name}')
          
          print('README structure generation complete!')
          "

      - name: Commit analysis results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action - URL Analyzer"
          
          if [ -n "$(git status --porcelain)" ]; then
            git add .
            ANALYSIS_COUNT=$(ls url_analysis_*.json | wc -l)
            LATEST_ANALYSIS=$(ls url_analysis_*.json | tail -1)
            git commit -m "feat: URL analysis and structure detection

- Analyzed $ANALYSIS_COUNT URL configurations
- Generated README structure for newspapers
- Detected file types and directory structures
- Created analysis report: $LATEST_ANALYSIS"
            git push
          else
            echo "No changes to commit"
          fi

      - name: Upload analysis artifacts
        uses: actions/upload-artifact@v4
        with:
          name: url-analysis-${{ github.run_number }}
          path: |
            url_analysis_*.json
          retention-days: 30

      - name: Summary
        run: |
          echo "## URL Analysis Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          
          ANALYSIS_FILE=$(ls url_analysis_*.json | tail -1)
          if [ -f "$ANALYSIS_FILE" ]; then
            TOTAL_NEWSPAPERS=$(python -c "import json; data=json.load(open('$ANALYSIS_FILE')); print(data['summary']['total_archives'])")
            TOTAL_FILES=$(python -c "import json; data=json.load(open('$ANALYSIS_FILE')); print(data['summary']['total_files'])")
            DIRECT_FILES=$(python -c "import json; data=json.load(open('$ANALYSIS_FILE')); print(data['summary']['direct_files'])")
            
            echo "| Total Newspapers | $TOTAL_NEWSPAPERS |" >> $GITHUB_STEP_SUMMARY
            echo "| Total Files | $TOTAL_FILES |" >> $GITHUB_STEP_SUMMARY
            echo "| Direct Files | $DIRECT_FILES |" >> $GITHUB_STEP_SUMMARY
            echo "| Analysis File | $ANALYSIS_FILE |" >> $GITHUB_STEP_SUMMARY
          fi