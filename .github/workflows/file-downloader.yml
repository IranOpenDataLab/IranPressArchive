name: File Downloader & Archive Builder

on:
  workflow_dispatch:
    inputs:
      newspaper_filter:
        description: 'Filter by newspaper name (leave empty for all)'
        required: false
        default: ''
        type: string
      max_files_per_newspaper:
        description: 'Maximum files to download per newspaper'
        required: false
        default: '100'
        type: string
      force_redownload:
        description: 'Force redownload existing files'
        required: false
        default: false
        type: boolean
  workflow_run:
    workflows: ["URL Analyzer & Structure Detection"]
    types:
      - completed
  schedule:
    # Run daily at 4 AM UTC (after URL analysis)
    - cron: '0 4 * * *'

permissions:
  contents: write
  actions: read

env:
  MAX_FILE_SIZE: 52428800  # 50MB
  DOWNLOAD_TIMEOUT: 300    # 5 minutes
  MAX_RETRIES: 3

jobs:
  download-files:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' || github.event_name == 'schedule' }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml requests beautifulsoup4 urllib3

      - name: Create analysis from urls.yml
        run: |
          echo "üì• Creating analysis from urls.yml..."
          
          cat > create_analysis.py << 'EOF'
          import yaml
          import json
          from datetime import datetime
          import os
          
          with open('urls.yml', 'r', encoding='utf-8') as f:
              config = yaml.safe_load(f)
          
          analysis = {
              'analysis_date': datetime.now().isoformat(),
              'newspapers': {},
              'summary': {'total_archives': 0, 'total_files': 0}
          }
          
          for archive in config.get('archives', []):
              archive_name = archive.get('folder', 'unknown')
              newspaper_info = {
                  'title_fa': archive.get('title_fa', ''),
                  'title_en': archive.get('title_en', ''),
                  'folder': archive_name,
                  'category': archive.get('category', 'old-newspaper'),
                  'description': archive.get('description', ''),
                  'newspaper_name': archive.get('source_info', {}).get('newspaper_name', ''),
                  'files': [],
                  'years': {}
              }
              
              for year, urls in archive.get('years', {}).items():
                  year_files = []
                  for i, url in enumerate(urls):
                      file_info = {
                          'url': url,
                          'type': 'direct_file',
                          'year': year,
                          'filename': f'{archive_name}_{i+1:03d}.pdf'
                      }
                      year_files.append(file_info)
                      newspaper_info['files'].append(file_info)
                      analysis['summary']['total_files'] += 1
                  
                  newspaper_info['years'][year] = year_files
              
              analysis['newspapers'][archive_name] = newspaper_info
              analysis['summary']['total_archives'] += 1
          
          analysis_file = f'url_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
          with open(analysis_file, 'w', encoding='utf-8') as f:
              json.dump(analysis, f, ensure_ascii=False, indent=2)
          
          print(f'Created analysis file: {analysis_file}')
          
          # Set environment variable
          with open(os.environ['GITHUB_ENV'], 'a') as f:
              f.write(f'ANALYSIS_FILE={analysis_file}\n')
          EOF
          
          python create_analysis.py

      - name: Download files and build archives
        env:
          NEWSPAPER_FILTER: ${{ github.event.inputs.newspaper_filter }}
          MAX_FILES_PER_NEWSPAPER: ${{ github.event.inputs.max_files_per_newspaper || '100' }}
          FORCE_REDOWNLOAD: ${{ github.event.inputs.force_redownload || 'false' }}
        run: |
          echo "üì• Starting file download and archive building..."
          
          cat > download_files.py << 'EOF'
          import json
          import os
          import requests
          import time
          from pathlib import Path
          from urllib.parse import urlparse
          import sys
          from datetime import datetime
          
          class FileDownloader:
              def __init__(self, max_file_size=50*1024*1024, timeout=300, max_retries=3):
                  self.max_file_size = max_file_size
                  self.timeout = timeout
                  self.max_retries = max_retries
                  self.session = requests.Session()
                  self.session.headers.update({
                      'User-Agent': 'Iranian Archive Workflow/1.0'
                  })
                  
                  self.stats = {
                      'downloaded': 0,
                      'skipped': 0,
                      'failed': 0,
                      'total_size': 0
                  }
              
              def download_file(self, url, target_path, force_redownload=False):
                  target_path = Path(target_path)
                  
                  if target_path.exists() and not force_redownload:
                      print(f'‚è≠Ô∏è  Skipping existing file: {target_path.name}')
                      self.stats['skipped'] += 1
                      return True
                  
                  target_path.parent.mkdir(parents=True, exist_ok=True)
                  
                  for attempt in range(self.max_retries):
                      try:
                          print(f'üì• Downloading: {url} -> {target_path.name} (attempt {attempt + 1})')
                          
                          response = self.session.get(url, timeout=self.timeout, stream=True)
                          response.raise_for_status()
                          
                          content_type = response.headers.get('content-type', '').lower()
                          if 'pdf' not in content_type and 'application/octet-stream' not in content_type:
                              print(f'‚ö†Ô∏è  Warning: Unexpected content type: {content_type}')
                          
                          content_length = response.headers.get('content-length')
                          if content_length and int(content_length) > self.max_file_size:
                              print(f'‚ùå File too large: {content_length} bytes')
                              self.stats['failed'] += 1
                              return False
                          
                          total_size = 0
                          with open(target_path, 'wb') as f:
                              for chunk in response.iter_content(chunk_size=8192):
                                  if chunk:
                                      f.write(chunk)
                                      total_size += len(chunk)
                                      
                                      if total_size > self.max_file_size:
                                          print(f'‚ùå File size exceeded limit during download')
                                          target_path.unlink()
                                          self.stats['failed'] += 1
                                          return False
                          
                          print(f'‚úÖ Downloaded: {target_path.name} ({total_size} bytes)')
                          self.stats['downloaded'] += 1
                          self.stats['total_size'] += total_size
                          return True
                          
                      except requests.exceptions.RequestException as e:
                          print(f'‚ùå Download failed (attempt {attempt + 1}): {e}')
                          if attempt < self.max_retries - 1:
                              time.sleep(2 ** attempt)
                          else:
                              self.stats['failed'] += 1
                              return False
                      
                      except Exception as e:
                          print(f'‚ùå Unexpected error: {e}')
                          self.stats['failed'] += 1
                          return False
                  
                  return False
          
          # Get environment variables
          analysis_file = os.environ.get('ANALYSIS_FILE')
          newspaper_filter = os.environ.get('NEWSPAPER_FILTER', '')
          max_files_per_newspaper = int(os.environ.get('MAX_FILES_PER_NEWSPAPER', '100'))
          force_redownload = os.environ.get('FORCE_REDOWNLOAD', 'false').lower() == 'true'
          
          if not analysis_file or not os.path.exists(analysis_file):
              print('‚ùå Analysis file not found!')
              sys.exit(1)
          
          # Load analysis
          with open(analysis_file, 'r', encoding='utf-8') as f:
              analysis = json.load(f)
          
          downloader = FileDownloader()
          
          print(f'üöÄ Starting download process...')
          print(f'üìä Total newspapers: {len(analysis["newspapers"])}')
          print(f'üìä Total files: {analysis["summary"]["total_files"]}')
          if newspaper_filter:
              print(f'üîç Filter: {newspaper_filter}')
          print(f'üìä Max files per newspaper: {max_files_per_newspaper}')
          print(f'üîÑ Force redownload: {force_redownload}')
          print('-' * 50)
          
          # Process each newspaper
          for newspaper_name, info in analysis['newspapers'].items():
              if newspaper_filter and newspaper_filter.lower() not in newspaper_name.lower():
                  continue
              
              print(f'\nüì∞ Processing: {info["title_fa"]} ({newspaper_name})')
              
              newspaper_dir = Path(f'old-newspaper/{newspaper_name}')
              newspaper_dir.mkdir(parents=True, exist_ok=True)
              
              files_downloaded = 0
              for file_info in info['files'][:max_files_per_newspaper]:
                  if files_downloaded >= max_files_per_newspaper:
                      break
                  
                  url = file_info['url']
                  filename = file_info['filename']
                  target_path = newspaper_dir / filename
                  
                  success = downloader.download_file(url, target_path, force_redownload)
                  if success:
                      files_downloaded += 1
                  
                  time.sleep(0.5)
              
              print(f'üìä {newspaper_name}: {files_downloaded} files processed')
          
          # Print final statistics
          print('\n' + '=' * 50)
          print('üìä DOWNLOAD STATISTICS')
          print('=' * 50)
          print(f'‚úÖ Downloaded: {downloader.stats["downloaded"]} files')
          print(f'‚è≠Ô∏è  Skipped: {downloader.stats["skipped"]} files')
          print(f'‚ùå Failed: {downloader.stats["failed"]} files')
          print(f'üíæ Total size: {downloader.stats["total_size"] / 1024 / 1024:.1f} MB')
          print('=' * 50)
          
          # Save download report
          report = {
              'download_date': datetime.now().isoformat(),
              'analysis_file': analysis_file,
              'statistics': downloader.stats,
              'settings': {
                  'newspaper_filter': newspaper_filter,
                  'max_files_per_newspaper': max_files_per_newspaper,
                  'force_redownload': force_redownload
              }
          }
          
          report_file = f'download_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
          with open(report_file, 'w', encoding='utf-8') as f:
              json.dump(report, f, ensure_ascii=False, indent=2)
          
          print(f'üìÑ Download report saved: {report_file}')
          EOF
          
          python download_files.py

      - name: Update main README
        run: |
          echo "üìù Updating main README file..."
          
          cat > update_readme.py << 'EOF'
          import os
          import json
          from pathlib import Path
          from datetime import datetime
          
          # Find latest analysis and download report
          analysis_files = [f for f in os.listdir('.') if f.startswith('url_analysis_')]
          report_files = [f for f in os.listdir('.') if f.startswith('download_report_')]
          
          if not analysis_files:
              print('No analysis file found!')
              exit(1)
          
          latest_analysis = sorted(analysis_files)[-1]
          latest_report = sorted(report_files)[-1] if report_files else None
          
          with open(latest_analysis, 'r', encoding='utf-8') as f:
              analysis = json.load(f)
          
          download_stats = {}
          if latest_report:
              with open(latest_report, 'r', encoding='utf-8') as f:
                  report = json.load(f)
                  download_stats = report.get('statistics', {})
          
          # Generate main README content
          readme_lines = [
              '# ÿ¢ÿ±ÿ¥€åŸà ŸÖÿ∑ÿ®Ÿàÿπÿßÿ™ ÿß€åÿ±ÿßŸÜ / Iranian Press Archive',
              '',
              'ÿß€åŸÜ ŸÖÿÆÿ≤ŸÜ ÿ¥ÿßŸÖŸÑ ÿ¢ÿ±ÿ¥€åŸà ÿØ€åÿ¨€åÿ™ÿßŸÑ ÿ±Ÿàÿ≤ŸÜÿßŸÖŸá‚ÄåŸáÿß Ÿà ŸÜÿ¥ÿ±€åÿßÿ™ ÿß€åÿ±ÿßŸÜ€å ÿßÿ≥ÿ™ ⁄©Ÿá ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿÆŸàÿØ⁄©ÿßÿ± ÿ¨ŸÖÿπ‚Äåÿ¢Ÿàÿ±€å Ÿà ÿ≥ÿßÿ≤ŸÖÿßŸÜÿØŸá€å ÿ¥ÿØŸá‚ÄåÿßŸÜÿØ.',
              '',
              'This repository contains a digital archive of Iranian newspapers and publications, automatically collected and organized.',
              '',
              '## üìä ÿ¢ŸÖÿßÿ± ⁄©ŸÑ€å / General Statistics',
              '',
              f'- **ÿ™ÿπÿØÿßÿØ ŸÜÿ¥ÿ±€åÿßÿ™ / Total Publications**: {analysis["summary"]["total_archives"]}',
              f'- **ÿ™ÿπÿØÿßÿØ ⁄©ŸÑ ŸÅÿß€åŸÑ‚ÄåŸáÿß / Total Files**: {analysis["summary"]["total_files"]}'
          ]
          
          if download_stats:
              readme_lines.extend([
                  f'- **ŸÅÿß€åŸÑ‚ÄåŸáÿß€å ÿØÿßŸÜŸÑŸàÿØ ÿ¥ÿØŸá / Downloaded Files**: {download_stats.get("downloaded", 0)}',
                  f'- **ÿ≠ÿ¨ŸÖ ⁄©ŸÑ / Total Size**: {download_stats.get("total_size", 0) / 1024 / 1024:.1f} MB'
              ])
          
          readme_lines.extend([
              '',
              '## üì∞ ŸÜÿ¥ÿ±€åÿßÿ™ ŸÖŸàÿ¨ŸàÿØ / Available Publications',
              ''
          ])
          
          # List all newspapers
          for newspaper_name, info in analysis['newspapers'].items():
              newspaper_dir = Path(f'old-newspaper/{newspaper_name}')
              if newspaper_dir.exists():
                  file_count = len([f for f in newspaper_dir.glob('*.pdf')])
                  readme_lines.append(f'- **[{info["title_fa"]}](old-newspaper/{newspaper_name}/)** - {file_count} ŸÅÿß€åŸÑ')
          
          readme_lines.extend([
              '',
              '## üîÑ ÿ¢ÿÆÿ±€åŸÜ ÿ®ÿ±Ÿàÿ≤ÿ±ÿ≥ÿßŸÜ€å / Last Update',
              '',
              f'- **ÿ™ÿ¨ÿ≤€åŸá Ÿà ÿ™ÿ≠ŸÑ€åŸÑ URL Ÿáÿß / URL Analysis**: {analysis["analysis_date"]}'
          ])
          
          if latest_report:
              with open(latest_report, 'r', encoding='utf-8') as f:
                  report = json.load(f)
              readme_lines.append(f'- **ÿØÿßŸÜŸÑŸàÿØ ŸÅÿß€åŸÑ‚ÄåŸáÿß / File Download**: {report["download_date"]}')
          
          readme_lines.extend([
              '',
              '## ü§ñ ÿ≥€åÿ≥ÿ™ŸÖ ÿÆŸàÿØ⁄©ÿßÿ± / Automated System',
              '',
              'ÿß€åŸÜ ÿ¢ÿ±ÿ¥€åŸà ÿ®ÿß ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ GitHub Actions ÿ®Ÿá ÿµŸàÿ±ÿ™ ÿÆŸàÿØ⁄©ÿßÿ± ÿ®ÿ±Ÿàÿ≤ÿ±ÿ≥ÿßŸÜ€å ŸÖ€å‚Äåÿ¥ŸàÿØ:',
              '',
              '1. **ÿ™ÿ¨ÿ≤€åŸá Ÿà ÿ™ÿ≠ŸÑ€åŸÑ URL Ÿáÿß**: ŸáŸÅÿ™⁄Ø€å €å⁄©ÿ¥ŸÜÿ®Ÿá‚ÄåŸáÿß ÿ≥ÿßÿπÿ™ 3 ÿµÿ®ÿ≠ UTC',
              '2. **ÿØÿßŸÜŸÑŸàÿØ ŸÅÿß€åŸÑ‚ÄåŸáÿß**: ÿ±Ÿàÿ≤ÿßŸÜŸá ÿ≥ÿßÿπÿ™ 4 ÿµÿ®ÿ≠ UTC',
              '',
              'This archive is automatically updated using GitHub Actions:',
              '',
              '1. **URL Analysis**: Weekly on Sundays at 3 AM UTC',
              '2. **File Download**: Daily at 4 AM UTC',
              '',
              '## üìÅ ÿ≥ÿßÿÆÿ™ÿßÿ± ŸæŸàÿ¥Ÿá‚ÄåŸáÿß / Directory Structure',
              '',
              '```',
              'old-newspaper/',
              '‚îú‚îÄ‚îÄ neshat/          # ÿ¢ÿ±ÿ¥€åŸà ŸÜÿ¥ÿ±€åŸá ŸÜÿ¥ÿßÿ∑',
              '‚îú‚îÄ‚îÄ kayhan/          # ÿ¢ÿ±ÿ¥€åŸà ŸÜÿ¥ÿ±€åŸá ⁄©€åŸáÿßŸÜ',
              '‚îú‚îÄ‚îÄ tous/            # ÿ¢ÿ±ÿ¥€åŸà ŸÜÿ¥ÿ±€åŸá ÿ™Ÿàÿ≥',
              '‚îî‚îÄ‚îÄ ...',
              '```',
              '',
              '## üîó ŸÖŸÜÿßÿ®ÿπ / Sources',
              '',
              'ŸÅÿß€åŸÑ‚ÄåŸáÿß ÿßÿ≤ ŸÖŸÜÿßÿ®ÿπ ŸÖÿÆÿ™ŸÑŸÅ ÿ¨ŸÖÿπ‚Äåÿ¢Ÿàÿ±€å ÿ¥ÿØŸá‚ÄåÿßŸÜÿØ Ÿà ÿßÿ∑ŸÑÿßÿπÿßÿ™ ÿ™⁄©ŸÖ€åŸÑ€å ÿßÿ≤ Ÿà€å⁄©€å‚ÄåŸæÿØ€åÿß ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ¥ÿØŸá ÿßÿ≥ÿ™.',
              '',
              'Files are collected from various sources and supplementary information is extracted from Wikipedia.',
              '',
              '---',
              f'*ÿ¢ÿÆÿ±€åŸÜ ÿ®ÿ±Ÿàÿ≤ÿ±ÿ≥ÿßŸÜ€å ÿÆŸàÿØ⁄©ÿßÿ± / Last automatic update: {datetime.now().isoformat()}*'
          ])
          
          readme_content = '\n'.join(readme_lines)
          
          # Write main README
          with open('README.md', 'w', encoding='utf-8') as f:
              f.write(readme_content)
          
          print('‚úÖ Main README updated!')
          EOF
          
          python update_readme.py

      - name: Commit downloaded files
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action - File Downloader"
          
          if [ -n "$(git status --porcelain)" ]; then
            # Count new files
            NEW_FILES=$(git status --porcelain | grep "^A" | wc -l)
            MODIFIED_FILES=$(git status --porcelain | grep "^M" | wc -l)
            NEWSPAPER_COUNT=$(ls old-newspaper/ 2>/dev/null | wc -l)
            
            git add .
            git commit -m "feat: download and organize newspaper files

- Downloaded files for $NEWSPAPER_COUNT newspapers
- Added $NEW_FILES new files
- Updated $MODIFIED_FILES existing files
- Updated main archive index"
            git push
          else
            echo "No changes to commit"
          fi

      - name: Upload download artifacts
        uses: actions/upload-artifact@v4
        with:
          name: download-report-${{ github.run_number }}
          path: |
            download_report_*.json
          retention-days: 30

      - name: Summary
        run: |
          echo "## File Download Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          
          REPORT_FILE=$(ls download_report_*.json 2>/dev/null | tail -1)
          if [ -f "$REPORT_FILE" ]; then
            DOWNLOADED=$(python -c "import json; data=json.load(open('$REPORT_FILE')); print(data['statistics']['downloaded'])")
            SKIPPED=$(python -c "import json; data=json.load(open('$REPORT_FILE')); print(data['statistics']['skipped'])")
            FAILED=$(python -c "import json; data=json.load(open('$REPORT_FILE')); print(data['statistics']['failed'])")
            TOTAL_SIZE=$(python -c "import json; data=json.load(open('$REPORT_FILE')); print(f\"{data['statistics']['total_size'] / 1024 / 1024:.1f} MB\")")
            
            echo "| Downloaded Files | $DOWNLOADED |" >> $GITHUB_STEP_SUMMARY
            echo "| Skipped Files | $SKIPPED |" >> $GITHUB_STEP_SUMMARY
            echo "| Failed Files | $FAILED |" >> $GITHUB_STEP_SUMMARY
            echo "| Total Size | $TOTAL_SIZE |" >> $GITHUB_STEP_SUMMARY
            echo "| Report File | $REPORT_FILE |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Count directories created
          NEWSPAPERS=$(ls old-newspaper/ 2>/dev/null | wc -l)
          echo "| Newspapers | $NEWSPAPERS |" >> $GITHUB_STEP_SUMMARY